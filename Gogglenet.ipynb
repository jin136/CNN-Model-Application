{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import Sequential,Model,initializers,layers,Input\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1257 files belonging to 10 classes.\n",
      "Using 1132 files for training.\n",
      "Found 1257 files belonging to 10 classes.\n",
      "Using 125 files for validation.\n"
     ]
    }
   ],
   "source": [
    "dir=r'D:\\dataset\\butterfly\\train'\n",
    "train_ds=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "dir,\n",
    "labels=\"inferred\",\n",
    "label_mode=\"int\",\n",
    "class_names=None,\n",
    "color_mode=\"rgb\",\n",
    "batch_size=4,\n",
    "image_size=(224, 224),\n",
    "shuffle=True,\n",
    "seed=10,\n",
    "validation_split=0.1,\n",
    "subset='training',\n",
    "interpolation=\"gaussian\",\n",
    "follow_links=False,\n",
    "crop_to_aspect_ratio=False,)\n",
    "\n",
    "\n",
    "validation_ds=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "dir,\n",
    "labels=\"inferred\",\n",
    "label_mode=\"int\",\n",
    "class_names=None,\n",
    "color_mode=\"rgb\",\n",
    "batch_size=4,\n",
    "image_size=(224, 224),\n",
    "shuffle=True,\n",
    "seed=10,\n",
    "validation_split=0.1,\n",
    "subset='validation',\n",
    "interpolation=\"gaussian\",\n",
    "follow_links=False,\n",
    "crop_to_aspect_ratio=False,)\n",
    "\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255.)\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stem(input):\n",
    "    x=layers.Conv2D(filters=64,kernel_size=(7,7),strides=(2,2),padding='same',activation='relu')(input)\n",
    "    x=layers.MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n",
    "    x=layers.LayerNormalization()(x)\n",
    "    x=layers.Conv2D(filters=192,kernel_size=(1,1),strides=(1,1),padding='same',activation='relu')(x)\n",
    "    x=layers.Conv2D(filters=192,kernel_size=(3,3),strides=(1,1),padding='same',activation='relu')(x)\n",
    "    x=layers.LayerNormalization()(x)\n",
    "    x=layers.MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)    \n",
    "    return x\n",
    "\n",
    "\n",
    "def Inception_Block(x,f1,f2,f3,f4,f5,f6):\n",
    "\n",
    "    x1=layers.Conv2D(filters=f1,kernel_size=(1,1),strides=(1,1),padding='same',activation='relu')(x)\n",
    "          \n",
    "    x2=layers.Conv2D(filters=f2,kernel_size=(1,1),strides=(1,1),padding='same',activation='relu')(x)\n",
    "    x2=layers.Conv2D(filters=f3,kernel_size=(3,3),strides=(1,1),padding='same',activation='relu')(x2)\n",
    "\n",
    "    x3=layers.Conv2D(filters=f4 ,kernel_size=(1,1),strides=(1,1),padding='same',activation='relu')(x)\n",
    "    x3=layers.Conv2D(filters=f5,kernel_size=(5,5),strides=(1,1),padding='same',activation='relu')(x3)\n",
    "       \n",
    "    x4=layers.MaxPooling2D(pool_size=(3,3),strides=(1,1),padding='same')(x)\n",
    "    x4=layers.Conv2D(filters=f6,kernel_size=(1,1),strides=(1,1),padding='same',activation='relu')(x4)\n",
    "    \n",
    "    return tf.concat([x1,x2,x3,x4],axis=3)\n",
    "\n",
    "def AUX1(input):\n",
    "    ax=layers.AveragePooling2D(pool_size=(1,1),strides=(1,1))(input)\n",
    "    ax=layers.Conv2D(filters=128,kernel_size=(1,1),strides=(1,1),padding='same',activation='relu')(ax)\n",
    "    ax=layers.Flatten()(ax)\n",
    "    ax=layers.Dense(units=1024,activation='relu')(ax)\n",
    "    ax=layers.Dropout(0.7)(ax)\n",
    "    ax=layers.Dense(units=10,activation='softmax',name='ax1')(ax)\n",
    "    return ax\n",
    "\n",
    "def AUX2(input):\n",
    "    ax=layers.AveragePooling2D(pool_size=(1,1),strides=(1,1))(input)\n",
    "    ax=layers.Conv2D(filters=128,kernel_size=(1,1),strides=(1,1),padding='same',activation='relu')(ax)\n",
    "    ax=layers.Flatten()(ax)\n",
    "    ax=layers.Dense(units=1024,activation='relu')(ax)\n",
    "    ax=layers.Dropout(0.7)(ax)\n",
    "    ax=layers.Dense(units=10,activation='softmax',name='ax2')(ax)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def Classifier(input):\n",
    "    fc=layers.AveragePooling2D(pool_size=(7,7),strides=(1,1))(input)\n",
    "    fc=layers.Flatten()(fc)\n",
    "    fc=layers.Dense(units=10,activation='softmax',name='output')(fc)    \n",
    "    return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=Input(shape=(224,224,3))\n",
    "x=Stem(inputs)\n",
    "x=Inception_Block(x,256,256,256,256,256,256)\n",
    "x=Inception_Block(x,480,480,480,480,480,480)\n",
    "x=layers.MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n",
    "x=Inception_Block(x,512,512,512,512,512,512)\n",
    "\n",
    "ax1=AUX1(x)\n",
    "x=Inception_Block(x,512,512,512,512,512,512)\n",
    "x=Inception_Block(x,512,512,512,512,512,512)\n",
    "x=Inception_Block(x,528,528,528,528,528,528)\n",
    "\n",
    "ax2=AUX2(x)\n",
    "x=Inception_Block(x,832,832,832,832,832,832)\n",
    "x=layers.MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n",
    "x=Inception_Block(x,832,832,832,832,832,832)\n",
    "x=Inception_Block(x,1024,1024,1024,1024,1024,1024)\n",
    "output=Classifier(x)\n",
    "   \n",
    "model = tf.keras.models.Model(inputs,[output,ax1,ax2],name = 'GoogleNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rate=tf.keras.optimizers.schedules.ExponentialDecay(1e-3, 10000, 0.97, staircase=False, name=None)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_rate),loss='sparse_categorical_crossentropy' ,metrics=['acc'],loss_weights={'output':1, 'ax1':0., 'ax2':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_best_path=r'D:\\GitHub_repo\\CNN-Model-Study\\save_weights\\GoogleNet/'\n",
    "model_checkpoint_best=ModelCheckpoint(filepath=model_checkpoint_best_path,monitor='val_acc',save_best_only=True,save_weights_only=True,save_freq='epoch')\n",
    "early_stopping=EarlyStopping(monitor='val_acc',patience=30,min_delta=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1184/7500 [===>..........................] - ETA: 16:00:09 - loss: 2.3257 - output_loss: 2.3257 - ax1_loss: 2.3293 - ax2_loss: 2.3027 - output_acc: 0.1053 - ax1_acc: 0.1023 - ax2_acc: 0.1019"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,{'output':y_train,'ax1':y_train,'ax2':y_train},validation_data=(x_test,{'output':y_test,'ax1':y_test,'ax2':y_test}),epochs=10,batch_size=8,verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f63f62229035d384e646c189fd217ceea058ae56feeee8d9eb8b5a7f24aaefef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
