{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import Sequential,Model,initializers,layers,Input\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m (x_train,y_train),(x_test,y_test)\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mdatasets\u001b[39m.\u001b[39;49mcifar10\u001b[39m.\u001b[39;49mload_data()\n\u001b[0;32m      6\u001b[0m \u001b[39m# Normalization\u001b[39;00m\n\u001b[0;32m      7\u001b[0m x_train\u001b[39m=\u001b[39mx_train\u001b[39m/\u001b[39m\u001b[39m255.\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jin\\anaconda3\\envs\\Deep\\lib\\site-packages\\keras\\datasets\\cifar10.py:98\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m6\u001b[39m):\n\u001b[0;32m     94\u001b[0m     fpath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, \u001b[39m\"\u001b[39m\u001b[39mdata_batch_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i))\n\u001b[0;32m     95\u001b[0m     (\n\u001b[0;32m     96\u001b[0m         x_train[(i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m10000\u001b[39m : i \u001b[39m*\u001b[39m \u001b[39m10000\u001b[39m, :, :, :],\n\u001b[0;32m     97\u001b[0m         y_train[(i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m10000\u001b[39m : i \u001b[39m*\u001b[39m \u001b[39m10000\u001b[39m],\n\u001b[1;32m---> 98\u001b[0m     ) \u001b[39m=\u001b[39m load_batch(fpath)\n\u001b[0;32m    100\u001b[0m fpath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, \u001b[39m\"\u001b[39m\u001b[39mtest_batch\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m x_test, y_test \u001b[39m=\u001b[39m load_batch(fpath)\n",
      "File \u001b[1;32mc:\\Users\\jin\\anaconda3\\envs\\Deep\\lib\\site-packages\\keras\\datasets\\cifar.py:32\u001b[0m, in \u001b[0;36mload_batch\u001b[1;34m(fpath, label_key)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39m\"\"\"Internal utility for parsing CIFAR data.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39m    A tuple `(data, labels)`.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(fpath, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m---> 32\u001b[0m     d \u001b[39m=\u001b[39m cPickle\u001b[39m.\u001b[39;49mload(f, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbytes\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     33\u001b[0m     \u001b[39m# decode utf8\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     d_decoded \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(x_train,y_train),(x_test,y_test)=tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalization\n",
    "x_train=x_train/255.\n",
    "\n",
    "x_test=x_test/255.\n",
    "\n",
    "x_train.shape\n",
    "\n",
    "x_train = tf.image.resize(x_train, [224,224]) \n",
    "\n",
    "x_test = tf.image.resize(x_test, [224,224]) \n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(np.array(y_train))\n",
    "y_test = tf.keras.utils.to_categorical(np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1257 files belonging to 10 classes.\n",
      "Using 1132 files for training.\n",
      "Found 1257 files belonging to 10 classes.\n",
      "Using 125 files for validation.\n"
     ]
    }
   ],
   "source": [
    "dir=r'D:\\dataset\\butterfly\\train'\n",
    "train_ds=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "dir,\n",
    "labels=\"inferred\",\n",
    "label_mode=\"int\",\n",
    "class_names=None,\n",
    "color_mode=\"rgb\",\n",
    "batch_size=4,\n",
    "image_size=(224, 224),\n",
    "shuffle=True,\n",
    "seed=10,\n",
    "validation_split=0.1,\n",
    "subset='training',\n",
    "interpolation=\"gaussian\",\n",
    "follow_links=False,\n",
    "crop_to_aspect_ratio=False,)\n",
    "\n",
    "validation_ds=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "dir,\n",
    "validation_split=0.1,\n",
    "subset='validation',\n",
    "shuffle=True,\n",
    "seed=10,\n",
    "image_size=(224,224),\n",
    "batch_size=4)\n",
    "\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255.)\n",
    "\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1257 files belonging to 10 classes.\n",
      "Using 1132 files for training.\n",
      "Found 1257 files belonging to 10 classes.\n",
      "Using 125 files for validation.\n"
     ]
    }
   ],
   "source": [
    "dir=r'D:\\dataset\\butterfly\\train'\n",
    "train_ds=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "dir,\n",
    "labels=\"inferred\",\n",
    "label_mode=\"int\",\n",
    "class_names=None,\n",
    "color_mode=\"rgb\",\n",
    "batch_size=4,\n",
    "image_size=(224, 224),\n",
    "shuffle=True,\n",
    "seed=10,\n",
    "validation_split=0.1,\n",
    "subset='training',\n",
    "interpolation=\"gaussian\",\n",
    "follow_links=False,\n",
    "crop_to_aspect_ratio=False,)\n",
    "\n",
    "validation_ds=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "dir,\n",
    "validation_split=0.1,\n",
    "subset='validation',\n",
    "shuffle=True,\n",
    "seed=10,\n",
    "image_size=(224, 224),\n",
    "batch_size=4)\n",
    "\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255.)\n",
    "\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/device:CPU:0\"):\n",
    "    model=Sequential([\n",
    "        layers.Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),strides=(1,1),padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Conv2D(kernel_size=(3,3),filters=64,strides=(1,1),padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)),\n",
    "\n",
    "        layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Conv2D(filters=128,kernel_size=(3,3),strides=(1,1),padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)),\n",
    "    \n",
    "        layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Conv2D(filters=256,kernel_size=(3,3),strides=(1,1),padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(units=4096),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Dense(units=4096),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Dense(units=10,activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model,to_file='VGGnet_16.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_18 (Conv2D)          (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 224, 224, 64)     256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_18 (ReLU)             (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 224, 224, 64)     256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_19 (ReLU)             (None, 224, 224, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 112, 112, 64)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 112, 112, 128)    512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_20 (ReLU)             (None, 112, 112, 128)     0         \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 112, 112, 128)    512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_21 (ReLU)             (None, 112, 112, 128)     0         \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 112, 112, 128)    512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_22 (ReLU)             (None, 112, 112, 128)     0         \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 56, 56, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 56, 56, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_23 (ReLU)             (None, 56, 56, 256)       0         \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 56, 56, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_24 (ReLU)             (None, 56, 56, 256)       0         \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 56, 56, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_25 (ReLU)             (None, 56, 56, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 28, 28, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 200704)            0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4096)              822087680 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 840,798,154\n",
      "Trainable params: 840,795,594\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_best_path=r'D:\\GitHub_repo\\CNN-Model-Application\\save_weights\\VGG16/'\n",
    "model_checkpoint_best=ModelCheckpoint(filepath=model_checkpoint_best_path,monitor='val_acc',save_best_only=True,save_weights_only=True,save_freq='epoch')\n",
    "early_stopping=EarlyStopping(monitor='val_acc',patience=30,min_delta=1e-4)\n",
    "\n",
    "\n",
    "lr_rate=tf.keras.optimizers.schedules.ExponentialDecay(1e-3, 10000, 0.97, staircase=False, name=None)\n",
    "\n",
    "    \n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_rate),loss='sparse_categorical_crossentropy',metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "283/283 [==============================] - 1105s 4s/step - loss: 3.6718 - acc: 0.2765 - val_loss: 4.8010 - val_acc: 0.4000\n",
      "Epoch 2/50\n",
      " 30/283 [==>...........................] - ETA: 17:10 - loss: 2.7450 - acc: 0.4167"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/device:CPU:0\"):\n",
    "    history=model.fit(train_ds,validation_data=validation_ds,epochs=50,batch_size=4,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_checkpoint_best_path)\n",
    "test_loss,test_acc=model.evaluate(x=x_test,y=y_test,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['training','validation'],loc='upper left')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['training','validation'],loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f63f62229035d384e646c189fd217ceea058ae56feeee8d9eb8b5a7f24aaefef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
